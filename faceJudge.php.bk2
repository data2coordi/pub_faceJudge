<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>パーソナルカラー AI診断</title>
    <!--
	<link rel="stylesheet" href="faceJudge.css">
	-->
    <script type='text/javascript' src='./js/face-api.min.js'></script>
    <!--
    	<script type="module" src="JudgeFaceApri.js"></script>
	-->
	<meta name="viewport" content="width=480">
</head>
<body>

	<!--y edit-->
	
	<img src="./img/base_img_base.png" id="base_img_base" alt=""/>
	<img src="./img/base_img_photo.png" id="base_img_photo" alt=""/>
	<img src="./img/base_img_back.png" id="base_img_back" alt=""/>
	<img src="./img/base_img_up.png" id="base_img_up" alt=""/>
	<!--y edit file -->
	<label for="selectedFile" id="selectedFile" input 
		type="file" accept=".jpg,.gif,.png,image/gif,image/jpeg,image/png">test</label>
	<!--y edit img -->
	<!--
	<img id="start_img" src="./img/face_u1.jpg"></img>
	-->
	<img id="start_img" src="./img/face.jpg"></img>
	<img id="base_img" src=""></img>
	<img id="result_img" src=""></img>

	<!--y edit button -->
	<button type="button" id="prepare">prepare</input>
	<button type="button" id="analyze">analyze</input>

	<!--y edit-->
	<p id="msg"style="line-height: 100%"></p>

</body>
</html>

<?php
	$rgb_scopes_tbl_src  = new \SplFileObject("./rgb_scopes.csv");
	$rgb_scopes_tbl_src ->setFlags( \SplFileObject::READ_CSV );
	foreach ($rgb_scopes_tbl_src as $f) {
		if (!$f[0]==""){
			$rgb_scopes_tbl[$f[0].$f[1]] = array_slice($f, 0, 7);
		}
	}
?>

<script>
/*
let base_canvas = {};
let ratio = 1;
let rgb_scopes_tbl = JSON.parse('<?php echo $rgb_scopes_tbl ?>');
console.log(rgb_scopes_tbl);
*/
</script>

<script type="text/javascript">
	var elm = document.getElementById("selectedFile");
	elm.onchange = function(evt){
	var selectFiles = evt.target.files;
		if(selectFiles.length != 0) {
			var fr = new FileReader();
			fr.readAsDataURL(selectFiles[0]);
			fr.onload = function(evt) {
				
				document.getElementById('start_img').src =  fr.result ;
			}
		}
	}
</script>




<script type="text/javascript">
	function judge_color(arg_canvas, positions){
		const canvasCtx = arg_canvas.getContext('2d');
		// 変更したい色の範囲を決めておく
		const minColor = { r: 108, g: 0, b: 0 };
		const maxColor = { r: 255, g: 60, b: 60 };

		// コンテキストからデータ取得
		//@@@@@@ ここでチェック対象のポイントを設定する
		//頬は30と14の間
		//console.log(positions[30]["x"]);
		//console.log(positions[30]["y"]);
		//console.log(positions[14]["x"]);
		//30: Object { _x: 1532.1725353041365, _y: 2033.0179298605783 }
		//14: Object { _x: 1786.9302918949797, _y: 2024.1943157996996 }
		//ratio:0.124
		// x 1659 y 2033 > x 205.716 y 252.092
		//r:208,g:174,b:162

		const hoho_x = (positions[30]["x"] + (positions[14]["x"] - positions[30]["x"]) / 2 ) * ratio;
		const hoho_y = positions[30]["y"] * ratio;

		console.log("ratio:" + ratio);
		console.log("hoho_x:" + hoho_x);
		console.log("hoho_y:" + hoho_y);

		//const imageData = canvasCtx.getImageData(0, 0, arg_canvas.width, arg_canvas.height);
		const imageData = canvasCtx.getImageData(hoho_x, hoho_y,  1, 1 );
		const data      = imageData.data; // rgba、1バイト×4のデータ

		console.log("canvas image data start");
		console.log(data);
		console.log("canvas image data end");

		// ここに現在のピクセル情報を入れていく
		const currentColor = {};

		// 1ピクセルずつ確認していく
		for(let i = 0, len = data.length; i < len; i += 4) {
		  currentColor.r = data[i];
		  currentColor.g = data[i + 1];
		  currentColor.b = data[i + 2];
		  console.log("i:" + i);
		  console.log(currentColor.r)
		  console.log(currentColor.g)
		  console.log(currentColor.b)
		  // 指定したrgb内であれば黄色に変換する
		  if(_checkTargetColor(currentColor, minColor, maxColor)) {
		    data[i]     = 255;
		    data[i + 1] = 255;
		    data[i + 2] = 0;
		    // data[i + 3] = 0; => アルファ値なので、0にすれば透明になる
		  }
		}

		// ImageDataオブジェクトに、変更済みのRGBAデータ（変数data）を代入する
		imageData.data = data;

		// canvasに変更済みのImageDataオブジェクトを描画する
		canvasCtx.putImageData(imageData, 0, 0);
	}

	// 色の判定用の関数（引数：現在のピクセルのrgb、指定色の最小値、指定色の最大値）
	// 指定したrgb内であれば true を返す
	function _checkTargetColor(current, min, max) {
		if(min.r > current.r || current.r > max.r) return;
		if(min.g > current.g || current.g > max.g) return;
		if(min.b > current.b || current.b > max.b) return;
		return true;
	};


</script>

<script>

Promise.all([
  faceapi.nets.tinyFaceDetector.loadFromUri('models'),
  faceapi.nets.faceLandmark68TinyNet.loadFromUri('models'),
  faceapi.nets.faceLandmark68Net.load("models"),
  faceapi.nets.faceLandmark68Net.loadFromUri('models'),
  faceapi.nets.ssdMobilenetv1.loadFromUri('models')
]).then(initialize);

// Promise.all()の処理(モデル読み込み)全て成功後に実行
async function initialize(){
  // 「ファイル選択」'selectedFile'と「顔検出」'button'に関数を割り当てる
  
  const prepare = document.getElementById('prepare');
  prepare.addEventListener("click", after_prepare, false);

  const analyze = document.getElementById('analyze');
  analyze.addEventListener("click", facedetection, false);


  async function facedetection() {

    // 'myImg'から画像取得 canvas作成
    const base_img = document.getElementById('base_img');
    //const base_img = document.getElementById('img');
    base_canvas 	= await faceapi.createCanvasFromMedia(base_img);
    base_canvas_before  = await faceapi.createCanvasFromMedia(base_img);

    document.getElementById("msg").textContent = "\n顔検出中\n少し時間がかかります"
    // canvasのスタイル変更　筆を赤色(255, 0, 0)にする
    const base_ctx = base_canvas.getContext('2d');
    base_ctx.strokeStyle = "rgb(255, 0, 0)";
    // 顔検出 + 特徴点68点抽出
    //const detections = await faceapi.detectAllFaces(start_img, new faceapi.TinyFaceDetectorOptions()
    const detections = await faceapi.detectAllFaces(start_img
    			).withFaceLandmarks(true); // faceLandmark68'Tiny'Net のときtrue

    console.log("detection:");
    console.log(detections);

    // 検出した顔の数だけループ
    let positions = {};
    for (const detection of detections){
      // 検出領域を長方形で表示
      const box = detection.detection.box;
      base_ctx.strokeRect(box.x * ratio, box.y * ratio, box.width * ratio, box.height * ratio);
      // 特徴点を中心とした3*3の正方形を68点プロット
      for (const point of detection.landmarks.positions){
        base_ctx.strokeRect(point.x * ratio, point.y * ratio, 3, 3);
      }
      console.log("test1:positions")
      positions = detection.landmarks.positions;
      console.log(positions)
    }
    // canvasを画像に変換
    document.getElementById("msg").textContent = ""
    const imgSrc = base_canvas.toDataURL("image/jpeg",0.9);
    document.getElementById("result_img").src = imgSrc;

    console.log("test2:positions")
    console.log(positions)

    judge_color(base_canvas, positions);
  }


  //////////////////////////////////////////////////////////////////////////////
  async function after_prepare() {

	console.log('after_prepare');

	// 選択された画像を読み込む
	const start_img = document.getElementById('start_img')
	// 画像サイズを変更(長辺を800pixにする)
	const MIN_SIZE = 500;

	//let canvas = await faceapi.createCanvasFromMedia(img);
	let tmp_canvas = faceapi.createCanvasFromMedia(start_img);
	let ctx = tmp_canvas.getContext('2d');
	//let ratio = 1;
	if( Math.max(tmp_canvas.width, tmp_canvas.height) > MIN_SIZE) 
		ratio = MIN_SIZE / Math.max(tmp_canvas.width, tmp_canvas.height);

	tmp_canvas.width = tmp_canvas.width * ratio;
	tmp_canvas.height = tmp_canvas.height * ratio;

	ctx.drawImage(start_img,  0, 0, tmp_canvas.width, tmp_canvas.height);

	// 画像を'myImg'に送る
	const imgSrc = tmp_canvas.toDataURL("image/jpeg");
	//let base_img = document.getElementById('base_img').src = imgSrc;
	let base_img = document.getElementById('base_img');
	base_img.src = imgSrc;
	//base_img.style.visibility = 'visible';
	start_img.style.visibility = 'hidden';
  }



}



</script>



